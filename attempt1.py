# This enhanced record linkage script builds upon the previous version by integrating SBERT embeddings 
# with a Random Forest classifier for improved non-linear decision making. It uses a more natural text 
# format for SBERT encoding, applies compound blocking on both given name prefixes and surnames to 
# increase candidate coverage, and balances class weights to handle data imbalance. The modelâ€™s 
# performance is evaluated using precision, recall, and F1-score to measure matching accuracy.

import recordlinkage
import pandas as pd
import numpy as np
from recordlinkage.datasets import load_febrl1
from recordlinkage import precision, recall
from sentence_transformers import SentenceTransformer, util
from sklearn.ensemble import RandomForestClassifier # THE NEW CLASSIFIER
from sklearn.model_selection import train_test_split

# --- A. Data Loading and Preparation (CRM Proxy) ---

# Load FEBRL 1 data and true matches
df = load_febrl1()
true_matches = load_febrl1(return_links=True)[1] 

print(f"Total Records: {len(df)}")
print(f"True Duplicate Pairs: {len(true_matches)}")
print("-" * 30)

# --- 1. Prepare Text for SBERT (Simulating Name, Email, Company) ---

def create_contact_text(record):
    """Combines FEBRL fields into a single text string for SBERT encoding."""
    # Using a simple concatenation format for optimal SBERT performance
    name = f"{record['given_name']} {record['surname']}".strip()
    id_field = str(record.get('soc_sec_id', '')).replace('.0', '')
    address = record.get('address_1', '')
    
    # NEW FORMAT: Simple concatenation for better semantic embedding
    return f"{name} {id_field} {address}" 

df['contact_text'] = df.apply(create_contact_text, axis=1)

# --- 2. Encode all records using Sentence-Transformers ---
model = SentenceTransformer('all-MiniLM-L6-v2')
texts = df['contact_text'].tolist()
print("Encoding records with SBERT...")
embeddings = model.encode(texts, convert_to_tensor=True, show_progress_bar=False) 
print("Encoding complete.")
print("-" * 30)

# -------------------------------------------------------------------
# B. Blocking (Candidate Selection) - IMPROVED BLOCKING
# -------------------------------------------------------------------

indexer = recordlinkage.Index()

# Create the prefix column explicitly 
df['given_name_prefix'] = df['given_name'].str[:2]

# IMPROVEMENT: Use Compound Blocking (EITHER match on prefix OR on full surname)
indexer.block(left_on='given_name_prefix') # Block 1 (Catches J. Smith vs John Smith)
indexer.block(left_on='surname')           # Block 2 (Catches Michael Smith vs Mike Smith more broadly)

candidate_links = indexer.index(df)

print(f"Candidate Pairs after Compound Blocking: {len(candidate_links):,}")
print("-" * 30)

# -------------------------------------------------------------------
# C. Comparison (Feature Generation: String Metrics + SBERT)
# -------------------------------------------------------------------

compare_cl = recordlinkage.Compare()

# 1. Traditional String Metrics
compare_cl.string('given_name', 'given_name', method='jarowinkler', label='name_jw')
compare_cl.string('surname', 'surname', method='jarowinkler', label='surname_jw')
compare_cl.exact('date_of_birth', 'date_of_birth', label='dob_exact')
compare_cl.string('address_1', 'address_1', method='damerau_levenshtein', threshold=0.8, label='address_dl')

features = compare_cl.compute(candidate_links, df)

# --- Add SBERT Similarity as a new Feature ---
sbert_scores = []
df_index_map = {idx: i for i, idx in enumerate(df.index)}

for id1, id2 in candidate_links:
    emb1 = embeddings[df_index_map[id1]]
    emb2 = embeddings[df_index_map[id2]]
    
    sim = util.cos_sim(emb1, emb2).item()
    sbert_scores.append(sim)

sbert_series = pd.Series(sbert_scores, index=candidate_links, name='sbert_sim')
features = pd.concat([features, sbert_series], axis=1)

print(f"Features created for {len(features)} candidate pairs.")
print(f"Features: {features.columns.tolist()}")
print("-" * 30)

# -------------------------------------------------------------------
# D. Classification (Random Forest - THE OPTIMAL MODEL)
# -------------------------------------------------------------------

# 1. Create Training Labels (y) - The robust alignment fix
# This function is guaranteed to work regardless of your recordlinkage version
y_true = np.in1d(candidate_links, true_matches)

# 2. Split data for robust evaluation
X_train, X_test, y_train, y_test = train_test_split(
    features, y_true, test_size=0.3, random_state=42, stratify=y_true
)

# 3. Train the Classifier: Random Forest
# Random Forest handles non-linear relationships better than Logistic Regression
model_rf = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
model_rf.fit(X_train, y_train)

# 4. Predict Matches on the full candidate set
predictions = model_rf.predict(features)
final_matches = features.index[predictions]

print("Random Forest Classifier (SBERT-enhanced) Trained and Applied.")
print(f"Final Matches found (Random Forest Model): {len(final_matches)}")
print("-" * 30)

# -------------------------------------------------------------------
# E. Evaluation
# -------------------------------------------------------------------

precision_score = precision(true_matches, final_matches)
recall_score = recall(true_matches, final_matches)
f1_score = 2 * (precision_score * recall_score) / (precision_score + recall_score) if (precision_score + recall_score) else 0

print("--- Evaluation (Random Forest + SBERT) ---")
print(f"Baseline F1-Score (LogReg): 0.8359 (Recall: 0.7180)")
print(f"Precision (Safety): {precision_score:.4f}")
print(f"Recall (Completeness):    {recall_score:.4f}")
print(f"F1-Score (Overall Performance):  {f1_score:.4f}")